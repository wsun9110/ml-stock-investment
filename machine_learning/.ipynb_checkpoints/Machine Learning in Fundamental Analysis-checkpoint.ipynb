{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prescription-membrane",
   "metadata": {},
   "source": [
    "# Machine Learning in Fundamental Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daily-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import scale\n",
    "import tushare as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-visiting",
   "metadata": {},
   "source": [
    "## Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pro = ts.pro_api('c1811742dad5bc1457bbdc81d14ef9e05c4bd17abdf9591bd730ddc5')\n",
    "# ts.set_token('c1811742dad5bc1457bbdc81d14ef9e05c4bd17abdf9591bd730ddc5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "quarter = 3\n",
    "df_profit = ts.get_profit_data(year,quarter)\n",
    "df_op = ts.get_operation_data(year,quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-identifier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sitting-knowing",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only sh stocks and remove *ST stocks\n",
    "df_profit_600 = df_profit[(df_profit['code'].str.startswith('600') & (df_profit['name'].str.contains('ST') == False))]\n",
    "df_op_600 = df_op[(df_op['code'].str.startswith('600') & (df_op['name'].str.contains('ST') == False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stock code column as index and inner join two tables\n",
    "df_profit_600.set_index('code',inplace=True)\n",
    "df_op_600.set_index('code',inplace=True)\n",
    "\n",
    "# drop name column to avoid duplication when merging \n",
    "df_op_600.drop('name',axis=1,inplace=True)\n",
    "df_profit_op_600 = df_profit_600.join(df_op_600,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stock price, sh index as of Nov 30th and Sep 30th\n",
    "old_date='2021-09-30'\n",
    "new_date='2021-11-30'\n",
    "\n",
    "# get_hist_data returns a 1 line df each time, but what we need is a value\n",
    "# so we have to slice the correct column ['close'] which returns a series, then convert it to values which returns an array, and slice the first item \n",
    "# however if there is no data on that day, an empty series would be returned, and slicing the first item would throw an error, so we also need to make sure the series is not empty by checking its len\n",
    "df_profit_op_600['old_price'] = [ts.get_hist_data(i,old_date,old_date)['close'].values[0] \n",
    "                                 if len(ts.get_hist_data(i,old_date,old_date)['close']) !=0 else np.nan for i in df_profit_op_600.index]\n",
    "\n",
    "df_profit_op_600['new_price'] = [ts.get_hist_data(i,new_date,new_date)['close'].values[0] \n",
    "                                 if len(ts.get_hist_data(i,new_date,new_date)['close']) !=0 else np.nan for i in df_profit_op_600.index]\n",
    "\n",
    "\n",
    "df_profit_op_600['old_index'] = ts.get_hist_data('sh',old_date,old_date)['close'].values[0]\n",
    "df_profit_op_600['new_index'] = ts.get_hist_data('sh',new_date,new_date)['close'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the pct change rate for each stock and sh index\n",
    "df_profit_op_600['price_change'] = (df_profit_op_600['new_price'] - df_profit_op_600['old_price'])/df_profit_op_600['old_price']\n",
    "df_profit_op_600['index_change'] = (df_profit_op_600['new_index'] - df_profit_op_600['old_index'])/df_profit_op_600['old_index']\n",
    "\n",
    "# compare both rates, see if a stock beats the market or not, *1 turns true/false into 1/0\n",
    "df_profit_op_600['beat'] = (df_profit_op_600['price_change'] >= df_profit_op_600['index_change']) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless features\n",
    "df_profit_op_600 = df_profit_op_600.drop(['net_profits','business_income',\n",
    "                                          'arturndays','inventory_days','currentasset_days',\n",
    "                                          'old_price','new_price','old_sh_index','new_sh_index','price_change','index_change'],axis=1)\n",
    "\n",
    "# drop rows with missing values\n",
    "df_profit_op_600.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and lables\n",
    "X = df_profit_op_600.drop(['name','beat'], axis=1)\n",
    "y = df_profit_op_600['beat']\n",
    "X = scale(X)\n",
    "\n",
    "# split data into training and testing set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-cheat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "musical-affiliate",
   "metadata": {},
   "source": [
    "## Train and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a model svm\n",
    "# svm takes the nearest training data points (support vectors) of any class, and draw the boundary that has the largest distance to those support vectors, since in general the larger the margin the lower the generalization error of the classifier. \n",
    "# but since problems are usually not always perfectly separable with a boundary, so we allow some samples to be misclassified\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC()\n",
    "\n",
    "# train the model\n",
    "clf_svm.fit(X_train,y_train)\n",
    "\n",
    "# test the model\n",
    "clf_svm.score(X_test,y_test)\n",
    "\n",
    "# compute a 5-fold cross validation score on the whole dataset\n",
    "# it lets the model to train on a random part of the data and test it on the rest, repeat 5 times\n",
    "cross_val_score(clf_svm,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a model knn\n",
    "# knn makes predictions by figuring out what are the labels of the most similar samples, and use the average of those labels as y_pred\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# train the model\n",
    "clf_knn.fit(X_train,y_train)\n",
    "\n",
    "# test the model\n",
    "clf_knn.score(X_test,y_test)\n",
    "\n",
    "cross_val_score(clf_knn,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a model\n",
    "# a random forest has many decision trees (default 100 trees), each decision tree is trained and then used to predict the result, the final result is derived by taking the average of all trees' predictions(or by the majority rule)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "# train the model\n",
    "clf_rf.fit(X_train,y_train)\n",
    "\n",
    "# test the model\n",
    "clf_rf.score(X_test,y_test)\n",
    "\n",
    "cross_val_score(clf_rf,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-prairie",
   "metadata": {},
   "source": [
    "it is prominent that RandomForestClassifier has an overall higher accuracy (approaching 70%) than SVM and KNN. We can feed in Q4 data later when they are available to this model to predict which stocks could beat the market and construct an asset pool from which we can pick our targets and utilize our technical strategies to trade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-trout",
   "metadata": {},
   "source": [
    "## Save, load and predict\n",
    "Once we have trained our model, we want to save it so that we dont need to re-train it from scratch again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save a model\n",
    "with open('beat_clf.pkl','wb') as f:\n",
    "    pickle.dump(clf_rf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model\n",
    "with open('beat_clf.pkl','rb') as f:\n",
    "    clf_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_new.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-cream",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-entity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-poker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "square-tradition",
   "metadata": {},
   "source": [
    "## *Get more data (more dimensions and more samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-moldova",
   "metadata": {},
   "source": [
    "### Define data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(year,quarter):\n",
    "    '''\n",
    "    This function returns the stats data for all stocks at given year-quarter combinations\n",
    "    year = yyyy\n",
    "    quarter = 1,2,3 or 4\n",
    "    '''\n",
    "    df_profit = ts.get_profit_data(year,quarter)\n",
    "    df_op = ts.get_operation_data(year,quarter)\n",
    "    df_growth = ts.get_growth_data(year,quarter)\n",
    "    df_debt = ts.get_debtpaying_data(year,quarter)\n",
    "    df_cash = ts.get_cashflow_data(year,quarter)\n",
    "    \n",
    "    return df_profit,df_op,df_growth,df_debt,df_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stats(df):\n",
    "    '''\n",
    "    Filter only SH stocks that are not *ST\n",
    "    '''\n",
    "    new_df = df[(df['code'].str.startswith('600') & (df['name'].str.contains('ST') == False))]\n",
    "    new_df.set_index('code',inplace=True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stats(df,old_date,new_date,index_name='sh'):\n",
    "    '''\n",
    "    Return the stock price and specific index on given dates, and judge whether a stock beats the market\n",
    "    old_date/new_date = yyyy-mm-dd\n",
    "    '''\n",
    "    df['old_price'] = [ts.get_hist_data(i,old_date,old_date)['close'].values[0] if len(ts.get_hist_data(i,old_date,old_date)['close']) !=0 else np.nan for i in df.index]\n",
    "    df['new_price'] = [ts.get_hist_data(i,new_date,new_date)['close'].values[0] if len(ts.get_hist_data(i,new_date,new_date)['close']) !=0 else np.nan for i in df.index]\n",
    "    df['old_index'] = ts.get_hist_data(index_name,old_date,old_date)['close'].values[0]\n",
    "    df['new_index'] = ts.get_hist_data(index_name,new_date,new_date)['close'].values[0]\n",
    "    \n",
    "    df['price_change'] = (df['new_price'] - df['old_price'])/df['old_price']\n",
    "    df['index_change'] = (df['new_index'] - df['old_index'])/df['old_index']\n",
    "    \n",
    "    df['beat'] = (df['price_change'] >= df['index_change']) * 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stats(df):\n",
    "    '''\n",
    "    Drop useless columns and na rows\n",
    "    '''\n",
    "    new_df = df.drop(['net_profits','business_income','arturndays','inventory_days','currentasset_days','old_price','new_price','old_index','new_index','price_change','index_change'],axis=1)\n",
    "    new_df.dropna(inplace=True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-species",
   "metadata": {},
   "source": [
    "### Get 2021Q1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "quarter = 1\n",
    "stats_dfs = get_stats(year,quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_date='2021-03-31'\n",
    "new_date='2021-05-31'\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "for each_df in stats_dfs:\n",
    "    each_df_new = process_stats(each_df)\n",
    "    if main_df.empty:\n",
    "        main_df = each_df_new\n",
    "    else:\n",
    "        each_df_new.drop('name',axis=1,inplace=True)\n",
    "        main_df = main_df.join(each_df_new,how='inner')\n",
    "    \n",
    "main_df = add_stats(main_df,old_date,new_date,'sh')\n",
    "main_df = drop_stats(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv('df_2021Q1.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-torture",
   "metadata": {},
   "source": [
    "### Get 2021Q2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "quarter = 2\n",
    "stats_dfs = get_stats(year,quarter)\n",
    "\n",
    "old_date='2021-06-30'\n",
    "new_date='2021-09-30'\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "for each_df in stats_dfs:\n",
    "    each_df_new = process_stats(each_df)\n",
    "    if main_df.empty:\n",
    "        main_df = each_df_new\n",
    "    else:\n",
    "        each_df_new.drop('name',axis=1,inplace=True)\n",
    "        main_df = main_df.join(each_df_new,how='inner')\n",
    "    \n",
    "main_df = add_stats(main_df,old_date,new_date,'sh')\n",
    "main_df = drop_stats(main_df)\n",
    "\n",
    "main_df.to_csv('df_2021Q2.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-herald",
   "metadata": {},
   "source": [
    "### Get 2021Q3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "quarter = 3\n",
    "stats_dfs = get_stats(year,quarter)\n",
    "\n",
    "old_date='2021-09-30'\n",
    "new_date='2021-11-30'\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "for each_df in stats_dfs:\n",
    "    each_df_new = process_stats(each_df)\n",
    "    if main_df.empty:\n",
    "        main_df = each_df_new\n",
    "    else:\n",
    "        each_df_new.drop('name',axis=1,inplace=True)\n",
    "        main_df = main_df.join(each_df_new,how='inner')\n",
    "    \n",
    "main_df = add_stats(main_df,old_date,new_date,'sh')\n",
    "main_df = drop_stats(main_df)\n",
    "\n",
    "main_df.to_csv('df_2021Q3.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-conditions",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-missouri",
   "metadata": {},
   "source": [
    "### Reading data and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "atlantic-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs = ['Q1','Q2','Q3']\n",
    "batches = [pd.read_csv('df_2021'+i+'.csv',index_col=0,encoding='gbk') for i in Qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "french-intention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:1.00\n",
      "testing score:0.69\n",
      "cross val score:0.64\n",
      "training score:1.00\n",
      "testing score:0.65\n",
      "cross val score:0.65\n",
      "training score:1.00\n",
      "testing score:0.62\n",
      "cross val score:0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "for each_batch in batches:\n",
    "    X = each_batch.drop(['name','beat'], axis=1)\n",
    "    y = each_batch['beat']\n",
    "    X = scale(X)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "    \n",
    "    clf_rf.fit(X_train,y_train)\n",
    "    print(f'training score:{clf_rf.score(X_train,y_train):.2f}')\n",
    "    print(f'testing score:{clf_rf.score(X_test,y_test):.2f}')\n",
    "    print(f'cross val score:{cross_val_score(clf_rf,X,y).mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "casual-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:0.58\n",
      "testing score:0.64\n",
      "cross val score:0.65\n",
      "training score:0.41\n",
      "testing score:0.41\n",
      "cross val score:0.67\n",
      "training score:0.93\n",
      "testing score:0.95\n",
      "cross val score:0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf_svm = svm.SVC()\n",
    "\n",
    "for each_batch in batches:\n",
    "    X = each_batch.drop(['name','beat'], axis=1)\n",
    "    y = each_batch['beat']\n",
    "    X = scale(X)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "    \n",
    "    clf_svm.fit(X_train,y_train)\n",
    "    print(f'training score:{clf_rf.score(X_train,y_train):.2f}')\n",
    "    print(f'testing score:{clf_rf.score(X_test,y_test):.2f}')\n",
    "    print(f'cross val score:{cross_val_score(clf_rf,X,y).mean():.2f}')\n",
    "\n",
    "import pickle\n",
    "with open('beat_clf.pkl','wb') as f:\n",
    "    pickle.dump(clf_svm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "chinese-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:0.61\n",
      "testing score:0.63\n",
      "cross val score:0.64\n",
      "training score:0.38\n",
      "testing score:0.37\n",
      "cross val score:0.66\n",
      "training score:0.93\n",
      "testing score:0.92\n",
      "cross val score:0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "for each_batch in batches:\n",
    "    X = each_batch.drop(['name','beat'], axis=1)\n",
    "    y = each_batch['beat']\n",
    "    X = scale(X)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "    \n",
    "    clf_knn.fit(X_train,y_train)\n",
    "    print(f'training score:{clf_rf.score(X_train,y_train):.2f}')\n",
    "    print(f'testing score:{clf_rf.score(X_test,y_test):.2f}')\n",
    "    print(f'cross val score:{cross_val_score(clf_rf,X,y).mean():.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-terrain",
   "metadata": {},
   "source": [
    "In the above examples, we trained three batches of data(Q1,Q2,Q3) and tested them separately to evaluate which model performs better. On average, all models are having a similar level of prediction accuracy, so we can choose any one from them and save to local for our backtesting later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-graph",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
